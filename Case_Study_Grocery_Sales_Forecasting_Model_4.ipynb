{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bhathiya-pilanawithana/Case-study-Kaggle-Sales-Forecasting-Favorita/blob/main/Case_Study_Grocery_Sales_Forecasting_Model_4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VsE_JoZqKcdN"
      },
      "source": [
        "First login to your Kaggle account and register to the competetion. Then create a token in Kaggle to use the Kaggle API. This will download 'kaggle.json' file to the local computer. (Use instructions in Method-2, Step-2 in https://www.geeksforgeeks.org/how-to-import-kaggle-datasets-directly-into-google-colab/)\n",
        "\n",
        "Then upload the json file to the root folder and run the following cell. It will download the dataset as a .zip file to the root."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "4NvIfbFUq9HW"
      },
      "outputs": [],
      "source": [
        "! pip install -q kaggle\n",
        "!mkdir ~/.kaggle #If the folder exists, comment this!\n",
        "!cp kaggle.json ~/.kaggle/\n",
        "!chmod 600 /root/.kaggle/kaggle.json\n",
        "!kaggle competitions download -c favorita-grocery-sales-forecasting"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B_pyvkmwOreT"
      },
      "source": [
        "The following is to unzip the  downloaded main .zip file and unzip the indivigual 7zip files contained in the main .zip file."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a1w8NfQzIyXj",
        "outputId": "97f7c613-6a74-4467-df38-2bef37b15a26"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "7-Zip [64] 16.02 : Copyright (c) 1999-2016 Igor Pavlov : 2016-05-21\n",
            "p7zip Version 16.02 (locale=en_US.UTF-8,Utf16=on,HugeFiles=on,64 bits,2 CPUs Intel(R) Xeon(R) CPU @ 2.20GHz (406F0),ASM,AES-NI)\n",
            "\n",
            "Scanning the drive for archives:\n",
            "  0M Scan /content/data/\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1 file, 1898 bytes (2 KiB)\n",
            "\n",
            "Extracting archive: /content/data/holidays_events.csv.7z\n",
            "--\n",
            "Path = /content/data/holidays_events.csv.7z\n",
            "Type = 7z\n",
            "Physical Size = 1898\n",
            "Headers Size = 146\n",
            "Method = LZMA2:24k\n",
            "Solid = -\n",
            "Blocks = 1\n",
            "\n",
            "  0%\b\b\b\b    \b\b\b\bEverything is Ok\n",
            "\n",
            "Size:       22309\n",
            "Compressed: 1898\n",
            "\n",
            "7-Zip [64] 16.02 : Copyright (c) 1999-2016 Igor Pavlov : 2016-05-21\n",
            "p7zip Version 16.02 (locale=en_US.UTF-8,Utf16=on,HugeFiles=on,64 bits,2 CPUs Intel(R) Xeon(R) CPU @ 2.20GHz (406F0),ASM,AES-NI)\n",
            "\n",
            "Scanning the drive for archives:\n",
            "  0M Scan /content/data/\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1 file, 14315 bytes (14 KiB)\n",
            "\n",
            "Extracting archive: /content/data/items.csv.7z\n",
            "--\n",
            "Path = /content/data/items.csv.7z\n",
            "Type = 7z\n",
            "Physical Size = 14315\n",
            "Headers Size = 122\n",
            "Method = LZMA2:17\n",
            "Solid = -\n",
            "Blocks = 1\n",
            "\n",
            "  0%\b\b\b\b    \b\b\b\bEverything is Ok\n",
            "\n",
            "Size:       101841\n",
            "Compressed: 14315\n",
            "\n",
            "7-Zip [64] 16.02 : Copyright (c) 1999-2016 Igor Pavlov : 2016-05-21\n",
            "p7zip Version 16.02 (locale=en_US.UTF-8,Utf16=on,HugeFiles=on,64 bits,2 CPUs Intel(R) Xeon(R) CPU @ 2.20GHz (406F0),ASM,AES-NI)\n",
            "\n",
            "Scanning the drive for archives:\n",
            "  0M Scan /content/data/\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1 file, 3762 bytes (4 KiB)\n",
            "\n",
            "Extracting archive: /content/data/oil.csv.7z\n",
            "--\n",
            "Path = /content/data/oil.csv.7z\n",
            "Type = 7z\n",
            "Physical Size = 3762\n",
            "Headers Size = 122\n",
            "Method = LZMA2:24k\n",
            "Solid = -\n",
            "Blocks = 1\n",
            "\n",
            "  0%\b\b\b\b    \b\b\b\bEverything is Ok\n",
            "\n",
            "Size:       20580\n",
            "Compressed: 3762\n",
            "\n",
            "7-Zip [64] 16.02 : Copyright (c) 1999-2016 Igor Pavlov : 2016-05-21\n",
            "p7zip Version 16.02 (locale=en_US.UTF-8,Utf16=on,HugeFiles=on,64 bits,2 CPUs Intel(R) Xeon(R) CPU @ 2.20GHz (406F0),ASM,AES-NI)\n",
            "\n",
            "Scanning the drive for archives:\n",
            "  0M Scan /content/data/\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1 file, 648 bytes (1 KiB)\n",
            "\n",
            "Extracting archive: /content/data/stores.csv.7z\n",
            "--\n",
            "Path = /content/data/stores.csv.7z\n",
            "Type = 7z\n",
            "Physical Size = 648\n",
            "Headers Size = 130\n",
            "Method = LZMA2:12\n",
            "Solid = -\n",
            "Blocks = 1\n",
            "\n",
            "  0%\b\b\b\b    \b\b\b\bEverything is Ok\n",
            "\n",
            "Size:       1387\n",
            "Compressed: 648\n",
            "\n",
            "7-Zip [64] 16.02 : Copyright (c) 1999-2016 Igor Pavlov : 2016-05-21\n",
            "p7zip Version 16.02 (locale=en_US.UTF-8,Utf16=on,HugeFiles=on,64 bits,2 CPUs Intel(R) Xeon(R) CPU @ 2.20GHz (406F0),ASM,AES-NI)\n",
            "\n",
            "Scanning the drive for archives:\n",
            "  0M Scan /content/data/\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1 file, 4885065 bytes (4771 KiB)\n",
            "\n",
            "Extracting archive: /content/data/test.csv.7z\n",
            "--\n",
            "Path = /content/data/test.csv.7z\n",
            "Type = 7z\n",
            "Physical Size = 4885065\n",
            "Headers Size = 122\n",
            "Method = LZMA2:24\n",
            "Solid = -\n",
            "Blocks = 1\n",
            "\n",
            "  0%\b\b\b\b    \b\b\b\b  6% - test.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b               \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 13% - test.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b               \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 19% - test.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b               \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 26% - test.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b               \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 33% - test.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b               \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 39% - test.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b               \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 43% - test.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b               \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 46% - test.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b               \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 53% - test.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b               \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 59% - test.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b               \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 69% - test.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b               \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 76% - test.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b               \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 83% - test.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b               \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 89% - test.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b               \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 93% - test.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b               \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 96% - test.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b               \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 99% - test.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b               \b\b\b\b\b\b\b\b\b\b\b\b\b\b\bEverything is Ok\n",
            "\n",
            "Size:       126163026\n",
            "Compressed: 4885065\n",
            "\n",
            "7-Zip [64] 16.02 : Copyright (c) 1999-2016 Igor Pavlov : 2016-05-21\n",
            "p7zip Version 16.02 (locale=en_US.UTF-8,Utf16=on,HugeFiles=on,64 bits,2 CPUs Intel(R) Xeon(R) CPU @ 2.20GHz (406F0),ASM,AES-NI)\n",
            "\n",
            "Scanning the drive for archives:\n",
            "  0M Scan /content/data/\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1 file, 474092593 bytes (453 MiB)\n",
            "\n",
            "Extracting archive: /content/data/train.csv.7z\n",
            "--\n",
            "Path = /content/data/train.csv.7z\n",
            "Type = 7z\n",
            "Physical Size = 474092593\n",
            "Headers Size = 122\n",
            "Method = LZMA2:24\n",
            "Solid = -\n",
            "Blocks = 1\n",
            "\n",
            "  0%\b\b\b\b    \b\b\b\b  0% - train.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b  1% - train.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b  2% - train.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b  3% - train.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b  4% - train.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b  5% - train.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b  6% - train.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b  7% - train.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b  8% - train.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b  9% - train.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 10% - train.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 11% - train.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 12% - train.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 13% - train.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 14% - train.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 15% - train.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 16% - train.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 17% - train.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 18% - train.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 19% - train.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 20% - train.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 21% - train.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 22% - train.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 23% - train.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 24% - train.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 25% - train.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 26% - train.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 27% - train.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 28% - train.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 29% - train.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 30% - train.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 31% - train.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 32% - train.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 33% - train.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 34% - train.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 35% - train.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 36% - train.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 37% - train.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 38% - train.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 39% - train.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 40% - train.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 41% - train.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 42% - train.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 43% - train.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 44% - train.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 45% - train.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 46% - train.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 47% - train.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 48% - train.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 49% - train.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 50% - train.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 51% - train.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 52% - train.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 53% - train.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 54% - train.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 55% - train.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 56% - train.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 57% - train.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 58% - train.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 59% - train.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 60% - train.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 61% - train.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 62% - train.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 63% - train.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 64% - train.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 65% - train.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 66% - train.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 67% - train.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 68% - train.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 69% - train.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 70% - train.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 71% - train.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 72% - train.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 73% - train.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 74% - train.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 75% - train.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 76% - train.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 77% - train.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 78% - train.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 79% - train.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 80% - train.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 81% - train.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 82% - train.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 83% - train.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 84% - train.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 85% - train.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 86% - train.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 87% - train.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 88% - train.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 89% - train.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 90% - train.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 91% - train.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 92% - train.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 93% - train.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 94% - train.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 95% - train.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 96% - train.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 97% - train.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 98% - train.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 99% - train.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b100% 1\b\b\b\b\b\b      \b\b\b\b\b\bEverything is Ok\n",
            "\n",
            "Size:       4997452288\n",
            "Compressed: 474092593\n",
            "\n",
            "7-Zip [64] 16.02 : Copyright (c) 1999-2016 Igor Pavlov : 2016-05-21\n",
            "p7zip Version 16.02 (locale=en_US.UTF-8,Utf16=on,HugeFiles=on,64 bits,2 CPUs Intel(R) Xeon(R) CPU @ 2.20GHz (406F0),ASM,AES-NI)\n",
            "\n",
            "Scanning the drive for archives:\n",
            "  0M Scan /content/data/\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1 file, 219499 bytes (215 KiB)\n",
            "\n",
            "Extracting archive: /content/data/transactions.csv.7z\n",
            "--\n",
            "Path = /content/data/transactions.csv.7z\n",
            "Type = 7z\n",
            "Physical Size = 219499\n",
            "Headers Size = 138\n",
            "Method = LZMA2:1536k\n",
            "Solid = -\n",
            "Blocks = 1\n",
            "\n",
            "  0%\b\b\b\b    \b\b\b\bEverything is Ok\n",
            "\n",
            "Size:       1552637\n",
            "Compressed: 219499\n"
          ]
        }
      ],
      "source": [
        "import zipfile\n",
        "with zipfile.ZipFile('favorita-grocery-sales-forecasting.zip', 'r') as zip_ref:\n",
        "    zip_ref.extractall('data')\n",
        "\n",
        "!7z e /content/data/holidays_events.csv.7z -aoa\n",
        "!7z e /content/data/items.csv.7z -aoa\n",
        "!7z e /content/data/oil.csv.7z -aoa\n",
        "!7z e /content/data/stores.csv.7z -aoa\n",
        "!7z e /content/data/test.csv.7z -aoa\n",
        "!7z e /content/data/train.csv.7z -aoa\n",
        "!7z e /content/data/transactions.csv.7z -aoa"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install \"numpy<2.0.0\" --force-reinstall #CatBoost requires an older Numpy Version\n",
        "!pip install catboost --force-reinstall"
      ],
      "metadata": {
        "id": "AsS6ntHu2tLS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Reading Training CSV file\n",
        "\n",
        "The csv file 'train' is too large to be loaded fully into the compute with 12GB RAM. Therefore, the following two code cells use chunking to read the csv file chunk by chunk and appended or skip the chunk based on chunk's date range."
      ],
      "metadata": {
        "id": "1FOrSZoFt7ca"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "6vT1amJQMC3d"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import gc\n",
        "\n",
        "df_train_chunks = pd.read_csv(\n",
        "    'train.csv',\n",
        "    parse_dates = ['date'],\n",
        "    date_format = '%d/%m/%Y',\n",
        "    chunksize = 5000000,\n",
        "    dtype = {\n",
        "        'id': np.int64,\n",
        "        'store_nbr': np.int64,\n",
        "        'item_nbr': np.int64,\n",
        "        'unit_sales': np.float64,\n",
        "        },\n",
        "    low_memory = False\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y1g2qzBgMlIs",
        "outputId": "1f018a77-474b-4691-8103-a27f51283005"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Chunk in process: from 2013-01-01 00:00:00 to 2013-05-01 00:00:00\n",
            "\tSkipping.\n",
            "Chunk in process: from 2013-05-01 00:00:00 to 2013-08-21 00:00:00\n",
            "\tSkipping.\n",
            "Chunk in process: from 2013-08-21 00:00:00 to 2013-12-05 00:00:00\n",
            "\tSkipping.\n",
            "Chunk in process: from 2013-12-05 00:00:00 to 2014-03-06 00:00:00\n",
            "\tSkipping.\n",
            "Chunk in process: from 2014-03-06 00:00:00 to 2014-06-07 00:00:00\n",
            "\tSkipping.\n",
            "Chunk in process: from 2014-06-07 00:00:00 to 2014-09-01 00:00:00\n",
            "\tSkipping.\n",
            "Chunk in process: from 2014-09-01 00:00:00 to 2014-11-11 00:00:00\n",
            "\tSkipping.\n",
            "Chunk in process: from 2014-11-11 00:00:00 to 2015-01-25 00:00:00\n",
            "\tSkipping.\n",
            "Chunk in process: from 2015-01-25 00:00:00 to 2015-04-17 00:00:00\n",
            "\tSkipping.\n",
            "Chunk in process: from 2015-04-17 00:00:00 to 2015-06-27 00:00:00\n",
            "\tSkipping.\n",
            "Chunk in process: from 2015-06-27 00:00:00 to 2015-08-27 00:00:00\n",
            "\tSkipping.\n",
            "Chunk in process: from 2015-08-27 00:00:00 to 2015-10-22 00:00:00\n",
            "\tSkipping.\n",
            "Chunk in process: from 2015-10-22 00:00:00 to 2015-12-15 00:00:00\n",
            "\tSkipping.\n",
            "Chunk in process: from 2015-12-15 00:00:00 to 2016-02-08 00:00:00\n",
            "\tSkipping.\n",
            "Chunk in process: from 2016-02-08 00:00:00 to 2016-04-01 00:00:00\n",
            "\tSkipping.\n",
            "Chunk in process: from 2016-04-01 00:00:00 to 2016-05-23 00:00:00\n",
            "\tSkipping.\n",
            "Chunk in process: from 2016-05-23 00:00:00 to 2016-07-14 00:00:00\n",
            "\tSkipping.\n",
            "Chunk in process: from 2016-07-14 00:00:00 to 2016-09-04 00:00:00\n",
            "\tSkipping.\n",
            "Chunk in process: from 2016-09-04 00:00:00 to 2016-10-26 00:00:00\n",
            "\tSkipping.\n",
            "Chunk in process: from 2016-10-26 00:00:00 to 2016-12-15 00:00:00\n",
            "\tSkipping.\n",
            "Chunk in process: from 2016-12-15 00:00:00 to 2017-02-02 00:00:00\n",
            "\tSkipping.\n",
            "Chunk in process: from 2017-02-02 00:00:00 to 2017-03-22 00:00:00\n",
            "\tSkipping.\n",
            "Chunk in process: from 2017-03-22 00:00:00 to 2017-05-08 00:00:00\n",
            "\tSkipping.\n",
            "Chunk in process: from 2017-05-08 00:00:00 to 2017-06-24 00:00:00\n",
            "\tAppending.\n",
            "Chunk in process: from 2017-06-24 00:00:00 to 2017-08-11 00:00:00\n",
            "\tAppending.\n",
            "Chunk in process: from 2017-08-11 00:00:00 to 2017-08-15 00:00:00\n",
            "\tAppending.\n",
            "Final date range in data is from 2017-05-08 to 2017-08-15.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "# The initial_cutoff_date below will be used to check whether the chunk in process should be\n",
        "# appended to the final dataframe or skipped. Every chunk with maximum date larger than the\n",
        "# initial_cutoff_date will be appended to the final dataframe.\n",
        "\n",
        "initial_cutoff_date = pd.to_datetime('2017-06-01', format='%Y-%m-%d')\n",
        "df_all = pd.DataFrame()\n",
        "for chunk in df_train_chunks:\n",
        "  min_date = pd.to_datetime(chunk['date'].min(), format='%Y-%m-%d')\n",
        "  max_date = pd.to_datetime(chunk['date'].max(), format='%Y-%m-%d')\n",
        "  print(f'Chunk in process: from {min_date} to {max_date}')\n",
        "\n",
        "  if max_date > initial_cutoff_date:\n",
        "    print('\\tAppending.')\n",
        "    if df_all.empty:\n",
        "      df_all = chunk.copy()\n",
        "    else:\n",
        "      df_all = pd.concat([df_all, chunk]).reset_index(drop=True)\n",
        "  else:\n",
        "    print('\\tSkipping.')\n",
        "\n",
        "del(chunk)\n",
        "\n",
        "min_date = df_all['date'].min()\n",
        "max_date = df_all['date'].max()\n",
        "print(f'Final date range in data is from {min_date} to {max_date}.')\n",
        "\n",
        "df_all['date'] = pd.to_datetime(df_all['date'], format='%Y-%m-%d') #Convert Date column\n",
        "df_all = df_all.drop('id', axis=1) #ID column is not required\n",
        "\n",
        "df_all['store_nbr'] = df_all['store_nbr'].astype('int32') #Downcast into int32 to save memory\n",
        "df_all['item_nbr'] = df_all['item_nbr'].astype('int32') #Downcast into int32 to save memory\n",
        "\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Check the number of unique items and stores and their availability through out the training period\n",
        "\n",
        "unique_dates = df_all['date'].unique()\n",
        "print(f'There are {len(unique_dates)} days in total.')\n",
        "all_dates = pd.date_range(min_date, max_date)\n",
        "missing_dates = np.setdiff1d(all_dates, unique_dates)\n",
        "print(f'There are {len(missing_dates)} missing dates between {min_date} and {max_date}')\n",
        "\n",
        "unique_stores = df_all['store_nbr'].unique()\n",
        "num_unique_stores = len(unique_stores)\n",
        "unique_items  = df_all['item_nbr'].unique()\n",
        "num_unique_items = len(unique_items)\n",
        "print(f'There are {num_unique_stores} unique stores and {num_unique_items} unique items')\n",
        "\n",
        "df_store_temp = df_all.groupby('date')['store_nbr'].nunique().reset_index()\n",
        "store_mismatch = df_store_temp[df_store_temp['store_nbr'] < num_unique_stores]['date'].values\n",
        "print(f'Not all the stores are available in {len(store_mismatch)} days.')\n",
        "del df_store_temp\n",
        "gc.collect();"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6-JXQ8tQb06b",
        "outputId": "cf34ce72-29d4-4635-d2f4-af486bf800d0"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There are 100 days in total.\n",
            "There are 0 missing dates between 2017-05-08 and 2017-08-15\n",
            "There are 54 unique stores and 3940 unique items\n",
            "Not all the stores are available in 1 days.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ar-YyF3kjX1f",
        "outputId": "f69ee6a6-466a-4aeb-ea18-b3b88565cb16"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "df_all = df_all[df_all['date'] >= pd.to_datetime('2017-06-28', format='%Y-%m-%d')].copy()\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "KjE1cG1edKO4"
      },
      "outputs": [],
      "source": [
        "#First date is removed due to possible improper chunking\n",
        "\n",
        "df_all = df_all[df_all['date'] != min_date].copy()\n",
        "gc.collect()\n",
        "\n",
        "min_date = pd.to_datetime(df_all['date'].min(), format='%Y-%m-%d')\n",
        "unique_dates = df_all['date'].unique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "-y9Wza7SHvT_"
      },
      "outputs": [],
      "source": [
        "# Turn target column unit_sales into log(unit_sales+1), since final performance\n",
        "# is evaluated based on log(unit_sales+1)\n",
        "\n",
        "df_all['unit_sales'] = np.log(np.clip(df_all['unit_sales'].values, a_min=0, a_max=None) + 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NZoeJqP0N3kV",
        "outputId": "c2d68496-2438-49ce-9c88-72af6728d3f9"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "#--Create a hold out test set for final testing----------------------------------------\n",
        "# Holdout test set is created such that it hold 16 days in the future for predictions.\n",
        "# That means test_date is selected to be 2017-07-30, so that dates from 2017-07-31 to\n",
        "# 2017-08-15 (16 days in total) is received for forecasting.\n",
        "\n",
        "test_date = pd.to_datetime('2017-07-30', format='%Y-%m-%d')\n",
        "df_holdout = df_all[(df_all['date'] >= test_date)].copy()\n",
        "df_all = df_all[(df_all['date'] < test_date)].copy()\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xtEQtBZGMn5K",
        "outputId": "4488463b-9e7e-46e5-9de7-74d26e93812f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "With selected maximum lag window lower cutoff date for training is 2017-07-01 00:00:00\n"
          ]
        }
      ],
      "source": [
        "# The training set lower cutoff date is selected such that the first \"lag_window_max\" days\n",
        "# are reserved to build lagged features.\n",
        "\n",
        "lag_window_max = 3\n",
        "train_cutoff_lower = min_date + pd.Timedelta(days = lag_window_max)\n",
        "print(f'With selected maximum lag window lower cutoff date for training is {train_cutoff_lower}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DneZKhHKcoDf",
        "outputId": "5866ecdd-1be7-4096-f79a-9db9f3184f8f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "# Hold-out test set requires history build lagged features.\n",
        "# Therefore, hold-out test set is appended with lag_window_max\n",
        "# number of days in history with respect to test_date\n",
        "\n",
        "df_holout_history = df_all[(df_all['date'] < test_date) & (df_all['date'] >= test_date - pd.Timedelta(days = lag_window_max))].copy()\n",
        "df_holdout = pd.concat([df_holdout, df_holout_history]).sort_values(by='date').reset_index(drop=True)\n",
        "\n",
        "del df_holout_history\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "Mpna5kzgKfPj"
      },
      "outputs": [],
      "source": [
        "#--Merge Store and Item dataframes-------------------------------------------\n",
        "def merge_item(df_all):\n",
        "  df_item = pd.read_csv('items.csv')\n",
        "  df_all = df_all.merge(df_item, on='item_nbr', how='left')\n",
        "\n",
        "  del df_item\n",
        "  gc.collect()\n",
        "\n",
        "  return df_all\n",
        "\n",
        "df_all = merge_item(df_all)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "EUoACdaDYcsG"
      },
      "outputs": [],
      "source": [
        "#--Merge Store Dataframe-----------------------------------------------------\n",
        "def merge_store(df_all):\n",
        "  df_store = pd.read_csv('stores.csv')\n",
        "  df_all = df_all.merge(df_store, on='store_nbr', how='left')\n",
        "\n",
        "  del df_store\n",
        "  gc.collect()\n",
        "\n",
        "  return df_all\n",
        "\n",
        "df_all = merge_store(df_all)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#--Merge other dataframes----------------------------------------------------\n",
        "def merge_oil_and_holidays(df_all):\n",
        "  df_oil = pd.read_csv('oil.csv')\n",
        "  df_oil['date'] = pd.to_datetime(df_oil['date'], format='%Y-%m-%d')\n",
        "  df_all = df_all.merge(df_oil, on='date', how='left')\n",
        "  df_all = df_all.rename(columns={'dcoilwtico': 'oil_price'})\n",
        "  df_all['oil_price'] = df_all['oil_price'].astype('float32')\n",
        "\n",
        "  df_holidays = pd.read_csv('holidays_events.csv')\n",
        "  df_holidays['date'] = pd.to_datetime(df_holidays['date'], format='%Y-%m-%d')\n",
        "  df_holidays = df_holidays[['date', 'type']]\n",
        "  df_holidays = df_holidays.drop_duplicates()\n",
        "  df_holidays = df_holidays.rename(columns={'type': 'holiday'})\n",
        "  df_all = df_all.merge(df_holidays, on='date', how='left')\n",
        "  df_all['holiday'] = df_all['holiday'].fillna('None')\n",
        "\n",
        "  del df_oil, df_holidays\n",
        "  gc.collect()\n",
        "\n",
        "  return df_all\n",
        "\n",
        "df_all = merge_oil_and_holidays(df_all)\n",
        "\n",
        "#--Create Time Features------------------------------------------------------\n",
        "def create_time_features(df_all):\n",
        "  df_all['day_of_month'] = df_all['date'].dt.day.astype('int32')\n",
        "  df_all['is_weekend'] = df_all['date'].dt.dayofweek.isin([5, 6]).astype('int32')\n",
        "  df_all[\"pay_day\"] = ((df_all[\"day_of_month\"] == 15) | (df_all['date'].dt.is_month_end)).astype('int32')\n",
        "  gc.collect();\n",
        "\n",
        "  return df_all\n",
        "\n",
        "df_all = create_time_features(df_all)\n",
        "#----------------------------------------------------------------------------"
      ],
      "metadata": {
        "id": "3fVTu17McpPv"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OFpDAl0_MssF"
      },
      "source": [
        "#### Model Set 4 - Aggregated Based on Item and Store Columns:\n",
        "\n",
        "This LightGBM model set is to produce predictions with item and store IDs.\n",
        "\n",
        "The model set is trained with data as below."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Model Set-1 is trained with data from {train_cutoff_lower} (>=) to test_date which is {test_date} (<).\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_SWpIaohI67M",
        "outputId": "729692e5-6237-4391-8cb0-b19bd2155232"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Set-1 is trained with data from 2017-07-01 00:00:00 (>=) to test_date which is 2017-07-30 00:00:00 (<).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-rIhbVGMOjeC",
        "outputId": "3afd50e1-f678-4c71-c884-b44727c17833"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "23"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "# Fill NA for missing items for each day and each store.\n",
        "\n",
        "def fill_missing_stores_items(df_all):\n",
        "  full_index = pd.MultiIndex.from_product(\n",
        "      [unique_dates, unique_stores, unique_items],\n",
        "      names=['date', 'store_nbr', 'item_nbr']\n",
        "  )\n",
        "  df_all = df_all.set_index(['date', 'store_nbr', 'item_nbr']).reindex(full_index).reset_index()\n",
        "  df_all['unit_sales'] = df_all['unit_sales'].fillna(0)\n",
        "\n",
        "  return df_all\n",
        "\n",
        "df_all = fill_missing_stores_items(df_all)\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "UlsfVRt6biho"
      },
      "outputs": [],
      "source": [
        "# Downcast columns to save memory\n",
        "\n",
        "#--Encode Catergorical Features as 'category' to Save RAM--------------------\n",
        "def encode_cat(df_all):\n",
        "  df_all['holiday'] = df_all['holiday'].astype('category')\n",
        "  df_all['city'] = df_all['city'].astype('category')\n",
        "  df_all['state'] = df_all['state'].astype('category')\n",
        "  df_all['type'] = df_all['type'].astype('category')\n",
        "  df_all['cluster'] = df_all['cluster'].astype('category')\n",
        "  df_all['family'] = df_all['family'].astype('category')\n",
        "  df_all['class'] = df_all['class'].astype('category')\n",
        "  df_all['perishable'] = df_all['perishable'].astype('category')\n",
        "\n",
        "  gc.collect()\n",
        "  return df_all\n",
        "\n",
        "df_all = encode_cat(df_all)\n",
        "#----------------------------------------------------------------------------"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OeyURnDBCjfg",
        "outputId": "919d0b65-0763-43e3-efb6-801b0af11547"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 10425240 entries, 0 to 10425239\n",
            "Data columns (total 17 columns):\n",
            " #   Column        Dtype         \n",
            "---  ------        -----         \n",
            " 0   date          datetime64[ns]\n",
            " 1   store_nbr     int32         \n",
            " 2   item_nbr      int32         \n",
            " 3   unit_sales    float64       \n",
            " 4   onpromotion   object        \n",
            " 5   family        category      \n",
            " 6   class         category      \n",
            " 7   perishable    category      \n",
            " 8   city          category      \n",
            " 9   state         category      \n",
            " 10  type          category      \n",
            " 11  cluster       category      \n",
            " 12  oil_price     float32       \n",
            " 13  holiday       category      \n",
            " 14  day_of_month  float64       \n",
            " 15  is_weekend    float64       \n",
            " 16  pay_day       float64       \n",
            "dtypes: category(8), datetime64[ns](1), float32(1), float64(4), int32(2), object(1)\n",
            "memory usage: 686.0+ MB\n"
          ]
        }
      ],
      "source": [
        "df_all.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "-r1NiP_zCne5"
      },
      "outputs": [],
      "source": [
        "def created_lagged_features_model4(df_all, lag_window_max):\n",
        "  df_all = df_all.sort_values(by=['date']).reset_index(drop=True)\n",
        "\n",
        "  for lag in range(1, lag_window_max + 1):\n",
        "    df_all[f'unit_sales_lag_{lag}'] = df_all.groupby(['store_nbr', 'item_nbr'], observed=False)['unit_sales'].shift(lag).astype('float32')\n",
        "    df_all[f'promotion_lag_{lag}']  = df_all.groupby(['store_nbr', 'item_nbr'], observed=False)['onpromotion'].shift(lag).astype('float32')\n",
        "\n",
        "  for lag in range(1, lag_window_max + 1):\n",
        "    df_all[f'unit_sales_diff_{lag}'] = df_all.groupby(['store_nbr', 'item_nbr'], observed=False)['unit_sales'].diff(lag).astype('float32')\n",
        "    df_all[f'promotion_diff_{lag}']  = df_all.groupby(['store_nbr', 'item_nbr'], observed=False)['onpromotion'].diff(lag).astype('float32')\n",
        "\n",
        "  for lag in range(2, lag_window_max + 1):\n",
        "    df_all[f'unit_sales_mean_{lag}'] = df_all.groupby(['store_nbr', 'item_nbr'], observed=False)['unit_sales'].rolling(window=lag).mean().reset_index(drop=True).astype('float32')\n",
        "    df_all[f'promotion_mean_{lag}']  = df_all.groupby(['store_nbr', 'item_nbr'], observed=False)['onpromotion'].rolling(window=lag).mean().reset_index(drop=True).astype('float32')\n",
        "\n",
        "  #for lag in range(2, lag_window_max + 1):\n",
        "  #  df_all[f'unit_sales_std_{lag}'] = df_all.groupby(['store_nbr', 'item_nbr'], observed=False)['unit_sales'].rolling(window=lag).std().reset_index(drop=True).astype('float32')\n",
        "  #  df_all[f'promotion_std_{lag}']  = df_all.groupby(['store_nbr', 'item_nbr'], observed=False)['onpromotion'].rolling(window=lag).std().reset_index(drop=True).astype('float32')\n",
        "\n",
        "  gc.collect()\n",
        "  return df_all\n",
        "\n",
        "df_all = created_lagged_features_model4(df_all, lag_window_max)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "I-vXB9vu3mvq"
      },
      "outputs": [],
      "source": [
        "# Obtain the targets for training for each feature vector.\n",
        "# Targets are obtained by shifting the unit_sales appropiately\n",
        "\n",
        "def create_targets_model4(df_all):\n",
        "  df_all = df_all.sort_values(by=['date']).reset_index(drop=True)\n",
        "  gc.collect()\n",
        "\n",
        "  for i in range(1,16+1):\n",
        "    df_all[f'target_{i}'] = df_all.groupby(['store_nbr', 'item_nbr'], observed=False)['unit_sales'].shift(-i).fillna(0)\n",
        "\n",
        "  gc.collect()\n",
        "  return df_all\n",
        "\n",
        "df_all = create_targets_model4(df_all)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training of the model"
      ],
      "metadata": {
        "id": "k0Dx2aO60EW2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hh_vGDNsHd7w",
        "outputId": "4fdb30a3-1f1f-4485-f191-0ef1d14d48ff"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training for unit sales 1 days in future.\n",
            "\tTraining set from 2017-07-01 00:00:00 to 2017-07-29 00:00:00\n",
            "\tTraining for unit sales 1 days in future completed.\n",
            "Training for unit sales 2 days in future.\n",
            "\tTraining set from 2017-07-01 00:00:00 to 2017-07-28 00:00:00\n",
            "\tTraining for unit sales 2 days in future completed.\n",
            "Training for unit sales 3 days in future.\n",
            "\tTraining set from 2017-07-01 00:00:00 to 2017-07-27 00:00:00\n",
            "\tTraining for unit sales 3 days in future completed.\n",
            "Training for unit sales 4 days in future.\n",
            "\tTraining set from 2017-07-01 00:00:00 to 2017-07-26 00:00:00\n",
            "\tTraining for unit sales 4 days in future completed.\n",
            "Training for unit sales 5 days in future.\n",
            "\tTraining set from 2017-07-01 00:00:00 to 2017-07-25 00:00:00\n",
            "\tTraining for unit sales 5 days in future completed.\n",
            "Training for unit sales 6 days in future.\n",
            "\tTraining set from 2017-07-01 00:00:00 to 2017-07-24 00:00:00\n",
            "\tTraining for unit sales 6 days in future completed.\n",
            "Training for unit sales 7 days in future.\n",
            "\tTraining set from 2017-07-01 00:00:00 to 2017-07-23 00:00:00\n",
            "\tTraining for unit sales 7 days in future completed.\n",
            "Training for unit sales 8 days in future.\n",
            "\tTraining set from 2017-07-01 00:00:00 to 2017-07-22 00:00:00\n",
            "\tTraining for unit sales 8 days in future completed.\n",
            "Training for unit sales 9 days in future.\n",
            "\tTraining set from 2017-07-01 00:00:00 to 2017-07-21 00:00:00\n",
            "\tTraining for unit sales 9 days in future completed.\n",
            "Training for unit sales 10 days in future.\n",
            "\tTraining set from 2017-07-01 00:00:00 to 2017-07-20 00:00:00\n",
            "\tTraining for unit sales 10 days in future completed.\n",
            "Training for unit sales 11 days in future.\n",
            "\tTraining set from 2017-07-01 00:00:00 to 2017-07-19 00:00:00\n",
            "\tTraining for unit sales 11 days in future completed.\n",
            "Training for unit sales 12 days in future.\n",
            "\tTraining set from 2017-07-01 00:00:00 to 2017-07-18 00:00:00\n",
            "\tTraining for unit sales 12 days in future completed.\n",
            "Training for unit sales 13 days in future.\n",
            "\tTraining set from 2017-07-01 00:00:00 to 2017-07-17 00:00:00\n",
            "\tTraining for unit sales 13 days in future completed.\n",
            "Training for unit sales 14 days in future.\n",
            "\tTraining set from 2017-07-01 00:00:00 to 2017-07-16 00:00:00\n",
            "\tTraining for unit sales 14 days in future completed.\n",
            "Training for unit sales 15 days in future.\n",
            "\tTraining set from 2017-07-01 00:00:00 to 2017-07-15 00:00:00\n",
            "\tTraining for unit sales 15 days in future completed.\n",
            "Training for unit sales 16 days in future.\n",
            "\tTraining set from 2017-07-01 00:00:00 to 2017-07-14 00:00:00\n",
            "\tTraining for unit sales 16 days in future completed.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "39"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "source": [
        "import lightgbm as lgb\n",
        "\n",
        "df_train = df_all[(df_all['date'] >= train_cutoff_lower) & (df_all['date'] < test_date)]\n",
        "\n",
        "model_parameters = {\n",
        "    'num_leaves': 40,\n",
        "    'min_data_in_leaf': 200,\n",
        "    'learning_rate': 0.02,\n",
        "    'feature_fraction': 0.8,\n",
        "    'bagging_fraction': 0.7,\n",
        "    'bagging_freq': 1,\n",
        "    \"max_bin\": 350,\n",
        "    'objective': 'regression',\n",
        "    'metric': 'l2',\n",
        "    \"verbose\": 0,\n",
        "    }\n",
        "\n",
        "cat_columns = ['family', 'class', 'perishable', 'city', 'state', 'type', 'cluster', 'holiday', 'day_of_month', 'is_weekend', 'pay_day']\n",
        "lag_cols = [f'unit_sales_lag_{i}' for i in range(1,lag_window_max+1)] + [f'promotion_lag_{i}' for i in range(1,lag_window_max+1)]\n",
        "diff_cols = [f'unit_sales_diff_{i}' for i in range(1,lag_window_max+1)] + [f'promotion_diff_{i}' for i in range(1,lag_window_max+1)]\n",
        "mean_cols = [f'unit_sales_mean_{i}' for i in range(2,lag_window_max+1)] + [f'promotion_mean_{i}' for i in range(2,lag_window_max+1)]\n",
        "#std_cols = [f'unit_sales_std_{i}' for i in range(2,lag_window_max+1)] + [f'promotion_std_{i}' for i in range(2,lag_window_max+1)]\n",
        "\n",
        "train_columns = cat_columns + lag_cols + diff_cols + mean_cols\n",
        "\n",
        "lgb_model_set1 = []\n",
        "for i in range(1,16+1):\n",
        "  gc.collect()\n",
        "  print(f'Training for unit sales {i} days in future.')\n",
        "  df_train_i = df_train[df_train['date'] <= test_date - pd.Timedelta(days = i)].copy()\n",
        "  train_min_date = df_train_i['date'].min()\n",
        "  train_max_date = df_train_i['date'].max()\n",
        "  print(f'\\tTraining set from {train_min_date} to {train_max_date}')\n",
        "  lgb_train = lgb.Dataset(df_train_i[train_columns], df_train_i[f'target_{i}'], categorical_feature=cat_columns)\n",
        "  gbm = lgb.train(\n",
        "    model_parameters,\n",
        "    lgb_train,\n",
        "    num_boost_round=200\n",
        "    )\n",
        "  lgb_model_set1.append(gbm)\n",
        "  print(f'\\tTraining for unit sales {i} days in future completed.')\n",
        "\n",
        "del df_train_i\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Inferencing for the Test Date"
      ],
      "metadata": {
        "id": "Yi-V6WNJ0Pgq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "GJ0ktjtHcXjq"
      },
      "outputs": [],
      "source": [
        "# Date preparation for inferencing. Using the same functions used for training data preparation\n",
        "\n",
        "\n",
        "df_test_day = df_holdout[df_holdout['date'] <= test_date].copy()\n",
        "df_future   = df_holdout[df_holdout['date'] >  test_date].copy()\n",
        "\n",
        "#del df_holdout\n",
        "gc.collect()\n",
        "\n",
        "df_test_day = merge_item(df_test_day)\n",
        "df_test_day = merge_store(df_test_day)\n",
        "df_test_day_model4 = df_test_day\n",
        "#del df_test_day #If there are other model sets, keep it.\n",
        "gc.collect()\n",
        "\n",
        "\n",
        "df_test_day_model4 = merge_oil_and_holidays(df_test_day_model4)\n",
        "\n",
        "df_test_day_model4 = create_time_features(df_test_day_model4)\n",
        "\n",
        "df_test_day_model4 = fill_missing_stores_items(df_test_day_model4)\n",
        "\n",
        "df_test_day_model4 = encode_cat(df_test_day_model4)\n",
        "\n",
        "df_test_day_model4 = created_lagged_features_model4(df_test_day_model4, lag_window_max)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "Zbc9nuY7hCX9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1a863e51-06c8-4c28-9f6d-fd2846c0886e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicting for unit sales 1 days in future.\n",
            "\tPredicting for unit sales 1 days in future completed.\n",
            "Predicting for unit sales 2 days in future.\n",
            "\tPredicting for unit sales 2 days in future completed.\n",
            "Predicting for unit sales 3 days in future.\n",
            "\tPredicting for unit sales 3 days in future completed.\n",
            "Predicting for unit sales 4 days in future.\n",
            "\tPredicting for unit sales 4 days in future completed.\n",
            "Predicting for unit sales 5 days in future.\n",
            "\tPredicting for unit sales 5 days in future completed.\n",
            "Predicting for unit sales 6 days in future.\n",
            "\tPredicting for unit sales 6 days in future completed.\n",
            "Predicting for unit sales 7 days in future.\n",
            "\tPredicting for unit sales 7 days in future completed.\n",
            "Predicting for unit sales 8 days in future.\n",
            "\tPredicting for unit sales 8 days in future completed.\n",
            "Predicting for unit sales 9 days in future.\n",
            "\tPredicting for unit sales 9 days in future completed.\n",
            "Predicting for unit sales 10 days in future.\n",
            "\tPredicting for unit sales 10 days in future completed.\n",
            "Predicting for unit sales 11 days in future.\n",
            "\tPredicting for unit sales 11 days in future completed.\n",
            "Predicting for unit sales 12 days in future.\n",
            "\tPredicting for unit sales 12 days in future completed.\n",
            "Predicting for unit sales 13 days in future.\n",
            "\tPredicting for unit sales 13 days in future completed.\n",
            "Predicting for unit sales 14 days in future.\n",
            "\tPredicting for unit sales 14 days in future completed.\n",
            "Predicting for unit sales 15 days in future.\n",
            "\tPredicting for unit sales 15 days in future completed.\n",
            "Predicting for unit sales 16 days in future.\n",
            "\tPredicting for unit sales 16 days in future completed.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "16"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "source": [
        "# Use the trained models to produce forecasts for each forecasting date.\n",
        "\n",
        "gc.collect()\n",
        "df_test_day_model4 = df_test_day_model4[df_test_day_model4['date'] == test_date].copy()\n",
        "\n",
        "for i in range(1,16+1):\n",
        "  print(f'Predicting for unit sales {i} days in future.')\n",
        "  df_test_day_model4[f'unit_sales_prediction_day+{i}'] = lgb_model_set1[i-1].predict(df_test_day_model4[train_columns])\n",
        "  print(f'\\tPredicting for unit sales {i} days in future completed.')\n",
        "\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Merge Predictions to the Future Dataframe"
      ],
      "metadata": {
        "id": "IfvqWGptFcdQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kiqv8_zGYxnD",
        "outputId": "fa012a09-bc0c-4664-f6e1-dd4a48bf86f1"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ],
      "source": [
        "prediction_columns = ['store_nbr', 'item_nbr'] + [ f'unit_sales_prediction_day+{i}' for i in range(1,16+1)] + ['unit_sales_mean_2', 'unit_sales_lag_1']\n",
        "df_test_day_model4 = df_test_day_model4[prediction_columns]\n",
        "\n",
        "df_future = merge_item(df_future)\n",
        "df_future = merge_store(df_future)\n",
        "df_future = df_future.merge(df_test_day_model4, on=['store_nbr', 'item_nbr'], how='left')\n",
        "#del df_test_day_model1_agg\n",
        "gc.collect()\n",
        "\n",
        "df_future['unit_sales_prediction_model4'] = 0.0\n",
        "for i in range(1,16+1):\n",
        "  df_future.loc[df_future['date'] == test_date + pd.Timedelta(days = i), 'unit_sales_prediction_model4'] = df_future.loc[df_future['date'] == test_date + pd.Timedelta(days = i), f'unit_sales_prediction_day+{i}']\n",
        "df_future = df_future.drop([f'unit_sales_prediction_day+{i}' for i in range(1,16+1)], axis=1)\n",
        "\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "Tt9ZGlW1NoNb"
      },
      "outputs": [],
      "source": [
        "#-For benchmarking and post processing-----------------------------------\n",
        "df_future = df_future.rename(columns={'unit_sales_lag_1': 'unit_sales_prediction_lag1', 'unit_sales_mean_2': 'unit_sales_prediction_mean2'})\n",
        "df_future['unit_sales_prediction_cnst0'] = 1\n",
        "#------------------------------------------------------------------------"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluate Model Performance and Benchmark"
      ],
      "metadata": {
        "id": "YimRrRbW02GS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9aYW2XnlnO13",
        "outputId": "8990a59b-a702-4056-f08d-e1f5339070f3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hold-out score for Model4       : 0.7195332182277393\n",
            "Hold-out score for Constant One : 1.1106297976379116\n",
            "Hold-out score for 2-day mean   : 1.8540883812596998\n"
          ]
        }
      ],
      "source": [
        "df_future['weight'] = df_future['perishable'].apply(lambda x: 1.25 if x == 1 else 1.0)\n",
        "\n",
        "df_future['model4: error^2 * weight'] = df_future['weight'] * ((df_future['unit_sales_prediction_model4'] - df_future['unit_sales'])**2)\n",
        "hold_out_score_model4 = np.sqrt(df_future['model4: error^2 * weight'].sum()/df_future['weight'].sum())\n",
        "print(f'Hold-out score for Model4       : {hold_out_score_model4}')\n",
        "\n",
        "df_future['cnst0: error^2 * weight'] = df_future['weight'] * ((df_future['unit_sales_prediction_cnst0'] - df_future['unit_sales'])**2)\n",
        "hold_out_score_model1 = np.sqrt(df_future['cnst0: error^2 * weight'].sum()/df_future['weight'].sum())\n",
        "print(f'Hold-out score for Constant One : {hold_out_score_model1}')\n",
        "\n",
        "df_future['mean2: error^2 * weight'] = df_future['weight'] * ((df_future['unit_sales_prediction_mean2'] - df_future['unit_sales'])**2)\n",
        "hold_out_score_model1 = np.sqrt(df_future['mean2: error^2 * weight'].sum()/df_future['weight'].sum())\n",
        "print(f'Hold-out score for 2-day mean   : {hold_out_score_model1}')"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "m0f1qmwgxWPx"
      },
      "execution_count": 24,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1AdkgIc1hCEyFHJHG2QIJ3ou22WECnXUp",
      "authorship_tag": "ABX9TyMHDSK8LxMPwzdO1ehtaMaX",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}